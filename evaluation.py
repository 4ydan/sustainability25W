import json
import config
from collections import defaultdict

from pycocoevalcap.cider.cider import Cider
from pycocoevalcap.bleu.bleu import Bleu
from pycocoevalcap.spice.spice import Spice

def import_data(quantization_mode):
    """
    Load the captions based on the specified quantization mode

    Args:
        quantization_mode: can be "none", "skip_vision_tower" or "full"

    Returns:
        Tuple containing the ground truth captions aswell as the one generated by the model
    """
    with open(f"{config.ANNOTATIONS_DIR}/captions_val2017.json", "r") as f:
        actual_captions_raw = json.load(f)["annotations"]

    with open(f"{config.OUTPUT_DIR}_{quantization_mode}/predicted_captions.json" , "r") as f:
        predicted_captions_raw = json.load(f)

    actual_captions = defaultdict(list)
    for ann in actual_captions_raw:
        actual_captions[ann["image_id"]].append(ann["caption"])

    predicted_captions = defaultdict(list)
    for p in predicted_captions_raw:
        img_id = int(str(p["image_id"]).lstrip("0"))  
        predicted_captions[img_id].append(p["caption"])

    common_ids = actual_captions.keys() & predicted_captions.keys()

    actual_captions = {i: actual_captions[i] for i in common_ids}
    predicted_captions = {i: predicted_captions[i] for i in common_ids}

    return (actual_captions, predicted_captions)


def calculate_cider_score(actual_captions, predicted_captions):
    """
    Calculates the cider score

    Args:
        actual_captions: the ground truth captions
        predicted_captions: the captions that were predicted by the model

    Returns:
        The average score produced across all captions
    """
    scorer = Cider()
    score, _ = scorer.compute_score(actual_captions, predicted_captions)
    return score


def calculate_bleu_score(actual_captions, predicted_captions):
    """
    Calculates the bleu score

    Args:
        actual_captions: the ground truth captions
        predicted_captions: the captions that were predicted by the model

    Returns:
        The average score produced across all captions
    """
    scorer = Bleu(4)  
    score, _ = scorer.compute_score(actual_captions, predicted_captions)
    return score[3]


def calculate_spice_score(actual_captions, predicted_captions):
    """
    Calculates the spice score if it is available (if java is installed)

    Args:
        actual_captions: the ground truth captions
        predicted_captions: the captions that were predicted by the model

    Returns:
        The average score produced across all captions
    """
    try:
        scorer = Spice()
        score, _ = scorer.compute_score(actual_captions, predicted_captions)
        return score
    except FileNotFoundError as e:
        print(f"SPICE could not be executed: {e}")
        return


def load_metrics():
    """
    Loads all the metrics.json for each quantization mode and saves them to a dictionairy

    Returns:
        A dictionairy that conatis all efficiency metrics
    """
    quantizations = ["none", "skip_vision_tower", "full"]
    all_metrics = {}

    for mode in quantizations:
        with open(f"{config.OUTPUT_DIR}_{mode}/metrics.json" , "r") as f:
          metrics =  json.load(f) 
        all_metrics[mode] = metrics

    return all_metrics

