{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run options\n",
    "num_images = 10\n",
    "device = \"cuda\"\n",
    "quantizations = [\"none\", \"skip_vision_tower\", \"full\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Quantization-Mode: none\n",
      "\u001b[32mINFO\u001b[0m: Starting inference - Quantization: none, Device: cuda, Images: 10\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: Using device: cuda, dtype: torch.bfloat16\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: Loading model...\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: Model weights loaded in 2.14s\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: Model ready (total: 2.75s)\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: Processing 10 images...\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: Completed 10 images in 0.00s\u001b[0m\n",
      "Running Quantization-Mode: skip_vision_tower\n",
      "\u001b[32mINFO\u001b[0m: Starting inference - Quantization: skip_vision_tower, Device: cuda, Images: 10\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: Using device: cuda, dtype: torch.float16\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: Loading model with skip_vision_tower quantization...\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: Model weights loaded in 8.41s\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: Model ready (total: 9.04s)\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: Processing 10 images...\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: Completed 10 images in 0.00s\u001b[0m\n",
      "Running Quantization-Mode: full\n",
      "\u001b[32mINFO\u001b[0m: Starting inference - Quantization: full, Device: cuda, Images: 10\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: Using device: cuda, dtype: torch.float16\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: Loading model with full quantization...\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: Model weights loaded in 7.91s\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: Model ready (total: 8.68s)\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: Processing 10 images...\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: Completed 10 images in 0.00s\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for quantization in quantizations:\n",
    "    print(f\"Running Quantization-Mode: {quantization}\")\n",
    "    !uv run main.py -q {quantization} -n {num_images} -d {device} --save-captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating mode: none\n",
      "{'testlen': 457, 'reflen': 130, 'guess': [457, 447, 437, 427], 'correct': [115, 43, 11, 2]}\n",
      "ratio: 3.5153846153575743\n",
      "SPICE could not be executed: [Errno 2] No such file or directory: 'java'\n",
      "Evaluating mode: skip_vision_tower\n",
      "{'testlen': 145, 'reflen': 128, 'guess': [145, 135, 125, 115], 'correct': [96, 44, 15, 6]}\n",
      "ratio: 1.13281249999115\n",
      "SPICE could not be executed: [Errno 2] No such file or directory: 'java'\n",
      "Evaluating mode: full\n",
      "{'testlen': 146, 'reflen': 125, 'guess': [146, 136, 126, 116], 'correct': [89, 39, 12, 3]}\n",
      "ratio: 1.167999999990656\n",
      "SPICE could not be executed: [Errno 2] No such file or directory: 'java'\n",
      "{'none': {'CIDEr': 0.4686390989279722, 'BLEU-4': 0.04110209752517103, 'SPICE': None}, 'skip_vision_tower': {'CIDEr': 0.6186018373313188, 'BLEU-4': 0.19171857782265467, 'SPICE': None}, 'full': {'CIDEr': 0.6000251567565085, 'BLEU-4': 0.14404865380830967, 'SPICE': None}}\n"
     ]
    }
   ],
   "source": [
    "from evaluation import calculate_cider_score, calculate_bleu_score, calculate_spice_score, import_data, load_metrics\n",
    "\n",
    "\n",
    "results = {}\n",
    "\n",
    "for mode in quantizations:\n",
    "    print(f\"Evaluating mode: {mode}\")\n",
    "    actual_captions, predicted_captions = import_data(mode)\n",
    "\n",
    "    cider_mean, _ = calculate_cider_score(actual_captions, predicted_captions)\n",
    "    bleu_mean, _  = calculate_bleu_score(actual_captions, predicted_captions)\n",
    "    spice_mean, _ = calculate_spice_score(actual_captions, predicted_captions) \n",
    "\n",
    "    results[mode] = {\n",
    "        \"CIDEr\": cider_mean,\n",
    "        \"BLEU-4\": bleu_mean,\n",
    "        \"SPICE\": spice_mean,\n",
    "    }\n",
    "\n",
    "print(results)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "none: {'peak_VRAM': 6054.29541015625, 'latency_per_image': 7.9536212682724, 'througput': 0.1256290401755195, 'model_size': 4284.425506591797}\n",
      "skip_vision_tower: {'peak_VRAM': 3109.45703125, 'latency_per_image': 3.505574083328247, 'througput': 0.28478013721985734, 'model_size': 4284.425506591797}\n",
      "full: {'peak_VRAM': 3203.17626953125, 'latency_per_image': 3.621748661994934, 'througput': 0.27570922470663617, 'model_size': 4284.425506591797}\n"
     ]
    }
   ],
   "source": [
    "for mode in quantizations:\n",
    "    print(f\"{mode}: {load_metrics(mode)}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
